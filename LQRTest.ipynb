{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "from networks import PiNetTV, QNetTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "horizon = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Optimality Benchmark\n",
    "\n",
    "Below we solve the LQR control problem exactly using CVXPY. First, set up the dynamics and costs. The goal is to navigate from $(2, 3)$ to origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 1], [0, 1]])\n",
    "B = np.array([[0], [1]])\n",
    "\n",
    "def dynamics(x, u):\n",
    "    return A @ x + B @ u\n",
    "\n",
    "def cost(x, u):\n",
    "    return cp.sum_squares(x)\n",
    "\n",
    "def terminal_cost(x):\n",
    "    return 100 * cost(x, np.array([[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually perform the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 53.475627070515856\n",
      "Optimal trajectory:\n",
      "[[ 2.0000e+00  5.0000e+00  1.9049e+00  7.1460e-01  2.3900e-01  2.4000e-03]\n",
      " [ 3.0000e+00 -3.0951e+00 -1.1903e+00 -4.7560e-01 -2.3660e-01  0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2.0, 3.0])\n",
    "\n",
    "x = cp.Variable((2, horizon + 1))\n",
    "u = cp.Variable((1, horizon))\n",
    "\n",
    "cstr = [x[:, 0] == x0]\n",
    "cost_func = 0\n",
    "\n",
    "for t in range(horizon):\n",
    "    cost_func += cost(x[:, t], u[:, t])\n",
    "    cstr += [x[:, t + 1] == dynamics(x[:, t], u[:, t])]\n",
    "    \n",
    "cost_func += terminal_cost(x[:, -1])\n",
    "\n",
    "problem = cp.Problem(cp.Minimize(cost_func), cstr)\n",
    "\n",
    "print(f'Optimal value: {problem.solve()}')\n",
    "print('Optimal trajectory:')\n",
    "print(x.value.round(decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Now we will learn a solution to the LQR problem via policy gradient. Below outlines two approaches. Both output Gaussian samples from distributions depending on the input state, but one outputs they differ in how they specify the distribution. The first model, `Controller`, outputs the diagonal of a matrix $A$ that specifies the covariance of the output as $AA^{\\mathrm{T}}$. The second outputs the log of the diagonal of the covariance matrix directly. The performance of these two parameterizations will be compared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrvs = 2\n",
    "\n",
    "def make_q_net(t):\n",
    "    return nn.Sequential(nn.Linear(4, 64), \n",
    "                         nn.ELU(), \n",
    "                         nn.Linear(64, 64), \n",
    "                         nn.ELU(), \n",
    "                         nn.Linear(64, ntrvs * 2))\n",
    "\n",
    "def make_pi_net(t):\n",
    "    return nn.Sequential(nn.Linear(ntrvs, 64), \n",
    "                         nn.ELU(), \n",
    "                         nn.Linear(64, 64), \n",
    "                         nn.ELU(), \n",
    "                         nn.Linear(64, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the function that will actually train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(pi_net, q_net, scenario, batch_size):\n",
    "    states = []\n",
    "    outputs = []\n",
    "    trvs = []\n",
    "    inputs = []\n",
    "    costs = []\n",
    "\n",
    "    log_probs = pt.zeros((horizon, batch_size))\n",
    "\n",
    "    for s in range(batch_size):\n",
    "        traj_states = [scenario.sample_initial_dist().reshape((-1, 1))]\n",
    "        traj_outputs = []\n",
    "        traj_trvs = []\n",
    "        traj_inputs = []\n",
    "        traj_costs = []\n",
    "        \n",
    "        trv = pt.zeros(ntrvs).reshape((-1, 1))\n",
    "\n",
    "        for t in range(horizon):\n",
    "            traj_outputs.append(scenario.sensor(traj_states[-1].flatten(), t).reshape((-1, 1)))            \n",
    "            \n",
    "            trv = q_net(traj_outputs[t].flatten(), trv.flatten(), t).reshape((-1, 1))\n",
    "            traj_trvs.append(trv)\n",
    "            \n",
    "            traj_inputs.append(pi_net(traj_trvs[t].flatten(), t).reshape((-1, 1)))\n",
    "            traj_costs.append(scenario.cost(traj_states[t].flatten(), traj_inputs[t].flatten(), t).reshape((-1, 1)))\n",
    "            traj_states.append(scenario.dynamics(traj_states[t].flatten(), traj_inputs[t], t).reshape((-1, 1)).detach())\n",
    "\n",
    "        traj_costs.append(scenario.terminal_cost(traj_states[-1].flatten()).reshape((-1, 1)))\n",
    "\n",
    "        states.append(pt.cat(traj_states, axis=1))\n",
    "        outputs.append(pt.cat(traj_outputs, axis=1))        \n",
    "        trvs.append(pt.cat(traj_trvs, axis=1))\n",
    "        inputs.append(pt.cat(traj_inputs, axis=1))\n",
    "        costs.append(pt.cat(traj_costs, axis=1))\n",
    "    \n",
    "    states = pt.stack(states, axis=2).detach()\n",
    "    outputs = pt.stack(outputs, axis=2).detach()\n",
    "    trvs = pt.stack(trvs, axis=2)\n",
    "    inputs = pt.stack(inputs, axis=2).detach()\n",
    "    costs = pt.stack(costs, axis=2)[0, :, :].detach()\n",
    "        \n",
    "    return states, outputs, trvs, inputs, costs\n",
    "\n",
    "def train(pi_net, q_net, scenario, epochs, batch_size, lr):\n",
    "    opt = pt.optim.Adam(list(pi_net.parameters()) + list(q_net.parameters()), lr=lr)\n",
    "\n",
    "    values = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):                \n",
    "        pi_log_probs = pt.zeros((horizon, batch_size))\n",
    "        q_log_probs = pt.zeros((horizon, batch_size))\n",
    "        \n",
    "        states, outputs, trvs, inputs, costs = rollout(pi_net, q_net, scenario, batch_size)        \n",
    "        values[epoch] = costs.sum(axis=0).mean()\n",
    "        \n",
    "        print(f'[{epoch}]\\t\\t{values[epoch]}') \n",
    "        \n",
    "        for s in range(batch_size):\n",
    "            trv = pt.zeros(ntrvs)\n",
    "            \n",
    "            for t in range(horizon):         \n",
    "                q_log_probs[t, s] = q_net.log_prob(trvs[:, t, s].detach(), outputs[:, t, s], trv.detach(), t)\n",
    "                pi_log_probs[t, s] = pi_net.log_prob(inputs[:, t, s], trvs[:, t, s], t)\n",
    "                trv = trvs[:, t, s]\n",
    "                \n",
    "        baseline = costs.sum(axis=0).mean()\n",
    "                \n",
    "        opt.zero_grad()        \n",
    "        loss = pt.mul(pi_log_probs.sum(axis=0), costs.sum(axis=0) - baseline).mean() + \\\n",
    "               pt.mul(q_log_probs.sum(axis=0), costs.sum(axis=0) - baseline).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        if epoch == epochs - 1:\n",
    "            return states, outputs, trvs, inputs, costs, values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will train the two models with the same hyperparameters and plot a smoothed version of their loss function over the training process. Note that other than the randomness of the control policy, the dyanmics of the system are deterministic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\t\t863.9296264648438\n",
      "[1]\t\t844.318603515625\n",
      "[2]\t\t756.8457641601562\n",
      "[3]\t\t687.4397583007812\n",
      "[4]\t\t707.6642456054688\n",
      "[5]\t\t644.6332397460938\n",
      "[6]\t\t757.3072509765625\n",
      "[7]\t\t791.5577392578125\n",
      "[8]\t\t672.6542358398438\n",
      "[9]\t\t747.8230590820312\n",
      "[10]\t\t648.2454833984375\n",
      "[11]\t\t752.33056640625\n",
      "[12]\t\t674.8875732421875\n",
      "[13]\t\t736.9977416992188\n",
      "[14]\t\t661.3593139648438\n",
      "[15]\t\t731.8026733398438\n",
      "[16]\t\t629.6526489257812\n",
      "[17]\t\t650.447021484375\n",
      "[18]\t\t636.896240234375\n",
      "[19]\t\t690.139404296875\n",
      "[20]\t\t727.3494262695312\n",
      "[21]\t\t670.5134887695312\n",
      "[22]\t\t639.4796752929688\n",
      "[23]\t\t603.8712768554688\n",
      "[24]\t\t693.7208862304688\n",
      "[25]\t\t657.169921875\n",
      "[26]\t\t637.8692016601562\n",
      "[27]\t\t623.0638427734375\n",
      "[28]\t\t665.5797119140625\n",
      "[29]\t\t629.7118530273438\n",
      "[30]\t\t606.3330688476562\n",
      "[31]\t\t576.2198486328125\n",
      "[32]\t\t661.5497436523438\n",
      "[33]\t\t536.869873046875\n",
      "[34]\t\t538.1593627929688\n",
      "[35]\t\t570.0509643554688\n",
      "[36]\t\t687.7838134765625\n",
      "[37]\t\t667.2703857421875\n",
      "[38]\t\t601.0037231445312\n",
      "[39]\t\t573.6373901367188\n",
      "[40]\t\t542.6990356445312\n",
      "[41]\t\t598.563232421875\n",
      "[42]\t\t586.6234741210938\n",
      "[43]\t\t540.0742797851562\n",
      "[44]\t\t528.35400390625\n",
      "[45]\t\t513.3690185546875\n",
      "[46]\t\t552.0497436523438\n",
      "[47]\t\t559.2241821289062\n",
      "[48]\t\t493.71319580078125\n",
      "[49]\t\t551.2285766601562\n",
      "[50]\t\t467.1532287597656\n",
      "[51]\t\t509.3424987792969\n",
      "[52]\t\t468.35125732421875\n",
      "[53]\t\t581.5333862304688\n",
      "[54]\t\t416.47052001953125\n",
      "[55]\t\t407.8309631347656\n",
      "[56]\t\t501.996337890625\n",
      "[57]\t\t478.1684265136719\n",
      "[58]\t\t472.1545715332031\n",
      "[59]\t\t458.493896484375\n",
      "[60]\t\t458.01495361328125\n",
      "[61]\t\t519.2908325195312\n",
      "[62]\t\t402.8101501464844\n",
      "[63]\t\t420.271484375\n",
      "[64]\t\t432.3265380859375\n",
      "[65]\t\t454.7303771972656\n",
      "[66]\t\t413.6450500488281\n",
      "[67]\t\t368.9193115234375\n",
      "[68]\t\t422.632568359375\n",
      "[69]\t\t396.9901428222656\n",
      "[70]\t\t382.0067138671875\n",
      "[71]\t\t380.4874572753906\n",
      "[72]\t\t379.0046081542969\n",
      "[73]\t\t356.3858337402344\n",
      "[74]\t\t350.0996398925781\n",
      "[75]\t\t347.5694580078125\n",
      "[76]\t\t390.9814758300781\n",
      "[77]\t\t415.2105407714844\n",
      "[78]\t\t273.6592102050781\n",
      "[79]\t\t285.164794921875\n",
      "[80]\t\t318.9781494140625\n",
      "[81]\t\t282.59063720703125\n",
      "[82]\t\t291.8570251464844\n",
      "[83]\t\t348.9896240234375\n",
      "[84]\t\t282.0429382324219\n",
      "[85]\t\t313.6715087890625\n",
      "[86]\t\t240.16831970214844\n",
      "[87]\t\t286.2388610839844\n",
      "[88]\t\t320.5015563964844\n",
      "[89]\t\t313.6216735839844\n",
      "[90]\t\t246.57688903808594\n",
      "[91]\t\t287.3724365234375\n",
      "[92]\t\t227.7412872314453\n",
      "[93]\t\t256.1123352050781\n",
      "[94]\t\t222.16488647460938\n",
      "[95]\t\t234.3332977294922\n",
      "[96]\t\t254.52920532226562\n",
      "[97]\t\t263.5853271484375\n",
      "[98]\t\t211.23648071289062\n",
      "[99]\t\t211.32574462890625\n",
      "[100]\t\t148.13577270507812\n",
      "[101]\t\t213.82603454589844\n",
      "[102]\t\t157.5843963623047\n",
      "[103]\t\t183.1958465576172\n",
      "[104]\t\t161.92855834960938\n",
      "[105]\t\t156.91586303710938\n",
      "[106]\t\t194.1335906982422\n",
      "[107]\t\t152.35980224609375\n",
      "[108]\t\t158.41220092773438\n",
      "[109]\t\t147.29722595214844\n",
      "[110]\t\t147.24559020996094\n",
      "[111]\t\t142.9390411376953\n",
      "[112]\t\t124.92330932617188\n",
      "[113]\t\t133.75172424316406\n",
      "[114]\t\t125.68370056152344\n",
      "[115]\t\t130.9528350830078\n",
      "[116]\t\t148.63980102539062\n",
      "[117]\t\t140.5693817138672\n",
      "[118]\t\t138.8653106689453\n",
      "[119]\t\t136.0693817138672\n",
      "[120]\t\t117.27552795410156\n",
      "[121]\t\t129.6630401611328\n",
      "[122]\t\t112.1892318725586\n",
      "[123]\t\t128.79954528808594\n",
      "[124]\t\t125.2114028930664\n",
      "[125]\t\t95.98360443115234\n",
      "[126]\t\t133.6336669921875\n",
      "[127]\t\t120.21997833251953\n",
      "[128]\t\t98.62171173095703\n",
      "[129]\t\t105.83045196533203\n",
      "[130]\t\t131.45094299316406\n",
      "[131]\t\t139.02122497558594\n",
      "[132]\t\t124.79727172851562\n",
      "[133]\t\t108.7269058227539\n",
      "[134]\t\t115.99588775634766\n",
      "[135]\t\t171.60247802734375\n",
      "[136]\t\t208.7970733642578\n",
      "[137]\t\t213.85433959960938\n",
      "[138]\t\t211.05784606933594\n",
      "[139]\t\t187.6473846435547\n",
      "[140]\t\t188.83343505859375\n",
      "[141]\t\t248.44908142089844\n",
      "[142]\t\t319.78826904296875\n",
      "[143]\t\t352.47991943359375\n",
      "[144]\t\t275.46356201171875\n",
      "[145]\t\t302.3872375488281\n",
      "[146]\t\t250.8335418701172\n",
      "[147]\t\t201.2794189453125\n",
      "[148]\t\t255.36341857910156\n",
      "[149]\t\t243.85382080078125\n",
      "[150]\t\t280.18719482421875\n",
      "[151]\t\t311.55035400390625\n",
      "[152]\t\t226.79476928710938\n",
      "[153]\t\t179.1258544921875\n",
      "[154]\t\t132.0364227294922\n",
      "[155]\t\t170.19383239746094\n",
      "[156]\t\t217.0093994140625\n",
      "[157]\t\t198.63888549804688\n",
      "[158]\t\t171.57037353515625\n",
      "[159]\t\t158.23007202148438\n",
      "[160]\t\t118.14469909667969\n",
      "[161]\t\t94.83782958984375\n",
      "[162]\t\t137.3397979736328\n",
      "[163]\t\t117.77294158935547\n",
      "[164]\t\t159.20162963867188\n",
      "[165]\t\t149.4413604736328\n",
      "[166]\t\t94.23668670654297\n",
      "[167]\t\t96.87535858154297\n",
      "[168]\t\t92.60867309570312\n",
      "[169]\t\t89.36297607421875\n",
      "[170]\t\t72.26620483398438\n",
      "[171]\t\t89.69697570800781\n",
      "[172]\t\t90.4204330444336\n",
      "[173]\t\t89.88197326660156\n",
      "[174]\t\t95.90434265136719\n",
      "[175]\t\t79.05255126953125\n",
      "[176]\t\t60.18621063232422\n",
      "[177]\t\t94.09384155273438\n",
      "[178]\t\t96.9274673461914\n",
      "[179]\t\t101.28357696533203\n",
      "[180]\t\t85.61431884765625\n",
      "[181]\t\t68.396728515625\n",
      "[182]\t\t66.62389373779297\n",
      "[183]\t\t86.31135559082031\n",
      "[184]\t\t87.39871978759766\n",
      "[185]\t\t86.94306945800781\n",
      "[186]\t\t98.10881805419922\n",
      "[187]\t\t85.6039810180664\n",
      "[188]\t\t54.493743896484375\n",
      "[189]\t\t84.2317886352539\n",
      "[190]\t\t84.32550048828125\n",
      "[191]\t\t116.95767211914062\n",
      "[192]\t\t80.03303527832031\n",
      "[193]\t\t59.25605392456055\n",
      "[194]\t\t66.96350860595703\n",
      "[195]\t\t75.46357727050781\n",
      "[196]\t\t87.1883316040039\n",
      "[197]\t\t110.13695526123047\n",
      "[198]\t\t90.95455932617188\n",
      "[199]\t\t85.29598999023438\n"
     ]
    }
   ],
   "source": [
    "from scenarios import LQRScenario, LavaScenario\n",
    "\n",
    "init_dist = Uniform(0, 5)\n",
    "sensor_dist = MultivariateNormal(pt.tensor([0.0, 0.0]), 0.01 * pt.eye(2))\n",
    "\n",
    "scenario = LavaScenario(lambda: init_dist.sample().item(), lambda: sensor_dist.sample())\n",
    "\n",
    "pt.manual_seed(0)\n",
    "\n",
    "pi_net = PiNetTV(make_pi_net, horizon)\n",
    "q_net = QNetTV(make_q_net, horizon)\n",
    "states, outputs, trvs, inputs, costs, values = train(pi_net, q_net, scenario, epochs=200, batch_size=100, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Tensorboard\n",
    "def smooth(scalars, weight):\n",
    "    last = scalars[0]\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "\n",
    "    return np.array(smoothed)\n",
    "\n",
    "smoothed_values = smooth(lvalues, 0.8)\n",
    "\n",
    "plt.plot(smoothed_values, label='Log Stds')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the log model's loss function converges more quickly than the other model. We now repeat this experiment, but with the initial condition of the system perturbed by noise drawn from $\\mathcal{N}(0, I)$ at the start of each rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
